{"cells":[{"cell_type":"markdown","metadata":{"id":"Ak6NTuN0bTWH"},"source":["# Deep Learning - Hyperparameter-Tuning\n","### *Facial age prediction - a SML Regression problem*"]},{"cell_type":"markdown","metadata":{"id":"4zzGTbT9bTWK"},"source":["# 1. References\n","\n","1. Introduction to the Keras Tuner, 2022, [official documentation](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n","2. Hyperband Tuner, [official documentation](https://keras.io/api/keras_tuner/tuners/hyperband/)\n","3. The base Tuner class, [official documentation](https://keras.io/api/keras_tuner/tuners/base_tuner/#tuner-class)\n","4. Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, 2018, [link article](https://jmlr.org/papers/v18/16-558.html)\n","5. HyperBand and BOHB: Understanding State of the Art Hyperparameter Optimization Algorithms, 2023, [blog link](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms)"]},{"cell_type":"markdown","metadata":{"id":"8e_ukk4abTWL"},"source":["# 2. Initial Treatment"]},{"cell_type":"markdown","metadata":{"id":"ndyocOXrbTWL"},"source":["## 2.1. Configurations and import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cxkpki9UpwJT"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8833,"status":"ok","timestamp":1680822173839,"user":{"displayName":"Jaime Kuei","userId":"13856333500777215893"},"user_tz":-60},"id":"iVuC8UG8Ki3n","outputId":"a19704be-323b-4ed1-f2b2-4ebeb342c8c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Installs the keras-tuner.\n","%pip install -q -U keras-tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KXDlQDyKMRZ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib\n","import matplotlib.image as mpimg\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow.keras import datasets\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras import layers, initializers, regularizers, optimizers, metrics, losses\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import backend as K\n","import keras_tuner as kt\n","\n","import os\n","import time\n","from pathlib import Path\n","import pickle\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"1rHPBBgwbTWN"},"source":["## 2.2. Auxiliary functions\n","\n","Collection of all user defined functions in this notebook. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGRynwnmbTWO"},"outputs":[],"source":["def train_best_model(model, training, test, epochs, batch_size):\n","  '''Train the best model found by a Keras Tuner and return its training history.\n","\n","  Args:\n","  --\n","      tuner (keras_tuner.engine.tuner.Tuner): A Keras Tuner object.\n","      training: The training data\n","      epochs (int): The number of epochs to train the model.\n","      batch_size (int): The batch size to use during training.\n","\n","  Returns:\n","  --\n","      A dictionary containing the training history of model.\n","  '''\n","  tf.keras.backend.clear_session()\n","  \n","  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","  print(f\"Start training of {model.__class__.__doc__}\")\n","  start = time.time()\n","\n","  history = model.fit(training, \n","                    epochs=epochs, \n","                    validation_data = test,\n","                    batch_size=batch_size,\n","                    callbacks=[early_stopping])\n","  print(\"Training time: {:.4f}s\".format(time.time() - start), end=\"\\n\\n\")\n","\n","  return history"]},{"cell_type":"markdown","metadata":{"id":"ehSs7AoebTWP"},"source":["## 2.3. Import dataset\n","\n","In the following all paths for the data import will be defined."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zq_rM1c34CWU"},"outputs":[],"source":["# Create folder for saving the hypertuned models.\n","!mkdir -p /content/drive/MyDrive/FacialAgeProject/models/hypertune"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXATE-oEbTWP"},"outputs":[],"source":["# Define the path, where the dataset should be saved\n","vm_path = \"/content\"\n","path = \"/content/drive/MyDrive/FacialAgeProject/\"\n","\n","data_path = os.path.join(path, 'data')\n","metadata_path = os.path.join(path, 'metadata')\n","dataset_path = os.path.join(data_path, \"facial_age_dataset_unsplit/\")\n","\n","preprocessed_path = os.path.join(data_path, 'facial_age_dataset_preprocessed')\n","\n","train_path = Path(os.path.join(preprocessed_path, 'train'))\n","test_path = Path(os.path.join(preprocessed_path, 'test'))\n","\n","metadata_csv_path = os.path.join(metadata_path, 'images_metadata.csv')\n","# metadata_csv_path = path + 'metadata/images_metadata.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9YkbrIbbTWP"},"outputs":[],"source":["# Image dataset with all the files configurations\n","images_df = pd.read_csv(metadata_csv_path)\n","images_df = images_df.sample(frac = 1.0).reset_index(drop = True)\n","images_df.head()\n","\n","# Train dataset to be used in the generator\n","train_file_names = pd.Series(list(train_path.glob(r'**/*.png')),name = 'file_names').astype('str')\n","train_file_names = train_file_names.apply(lambda x : x.split(\"/\")[-1])\n","train_images_df = images_df[images_df['file_name'].isin(train_file_names)]\n","train_images_df['file_name'] = '..' + f'{train_path}'+ '/' + train_images_df['age_label'].apply(lambda x : f\"{x:03d}\") + '/' + train_images_df['file_name']\n","# train_images_df['weights'] = train_images_df['age_label'].apply(lambda x : weights_dict[x])\n","\n","# Test dataset to be used in the generator\n","test_file_names = pd.Series(list(test_path.glob(r'**/*.png')),name = 'file_names').astype('str')\n","test_file_names = test_file_names.apply(lambda x : x.split('/')[-1])\n","test_images_df = images_df[images_df['file_name'].isin(test_file_names)]\n","test_images_df['file_name'] = '..' + f'{test_path}'+ '/' + test_images_df['age_label'].apply(lambda x : f\"{x:03d}\") + '/' + test_images_df['file_name']\n","\n","print('Train files and dataset lenght (must check) : {} vs {}'.format(len(train_file_names),train_images_df.shape[0]))\n","print('test files and dataset lenght (must check) : {} vs {}'.format(len(test_file_names),test_images_df.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9M6OoGfbTWQ"},"outputs":[],"source":["# Defining global variables\n","batch_size = 64\n","seed = 0\n","epochs = 10\n","input_shape = (None, 200, 200, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iF-CfQ5WKTe8"},"outputs":[],"source":["# Data generators and parameters\n","train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n","    validation_split=0.2\n",")\n","\n","test_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n","\n","generate_params = {\n","    'target_size' : (200,200),\n","    'color_mode' : 'rgb',\n","    'class_mode' : 'raw',\n","    'batch_size' : batch_size,\n","    'seed' : seed\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4411,"status":"ok","timestamp":1680822221277,"user":{"displayName":"Jaime Kuei","userId":"13856333500777215893"},"user_tz":-60},"id":"zdoKnjnIKVS3","outputId":"8b17c6f8-fb34-48c5-9aab-73ab6c2bccc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5607 validated image filenames.\n","Found 1401 validated image filenames.\n","Found 1752 validated image filenames.\n"]}],"source":["# Generators instances and data spliting\n","ds_train = train_generator.flow_from_dataframe(\n","    dataframe=train_images_df,\n","    x_col='file_name',\n","    y_col='age_label',\n","    #weight_col = 'weights',\n","    shuffle=True,\n","    subset='training',\n","    **generate_params\n",")\n","\n","ds_val = train_generator.flow_from_dataframe(\n","    dataframe=train_images_df,\n","    x_col='file_name',\n","    y_col='age_label',\n","    shuffle=True,\n","    subset='validation',\n","    **generate_params\n",")\n","\n","ds_test = test_generator.flow_from_dataframe(\n","    dataframe=test_images_df,\n","    x_col='file_name',\n","    y_col='age_label',\n","    **generate_params\n",")"]},{"cell_type":"markdown","metadata":{"id":"O2igS3UkbTWQ"},"source":["# 3. Hyperparameter-Tuning\n","\n","\n","\n","- In the following we will apply hyperparameter tuning for our best LeNet and AlexNet architectures\n","- Again here the details of both models:\n","  - LeNet-V7 Enhanced Architecture + L2 Regularization (0.001) + `Adam` optimizer \n","  - AlexNet-V4 Less Complex Architecture 1 + Dropout Rate (0.25) + L2 Regularization (0.001)\n","- As hyperparameter technique we will use `keras_tuner.Hyperband()`, we followed the following resources about configuration and implementation of hypeband using keras [1][2][3]\n","\n","\n","**Hyperband**\n","\n","Hyperband is a highly efficient hyperparameter optimization algorithm that has gained popularity in recent years. One of its key advantages is its speed, which makes it faster than RandomSearch and Bayesian Algorithms. \n","\n","It's based on SuccessiveHalving that: \n","> \"...uniformily allocate a budget to a set of hyperparameter configurations, evaluate the performance of all configurations, throw out the worst half, and repeat until one configuration remains. The algorithm allocates exponentially more resources to more promising configurations\" [4]\n","\n","And there's some studies that proves that can be: \n","> \"... 5x to 30x faster than popular Bayesian optimization algorithms ...\" [4]\n","\n","Others resources, also shown that: \n","> \"... HyperBand has better performance in comparison with random search.\" [5]\n"]},{"cell_type":"markdown","metadata":{"id":"U5jXvd-nbTWQ"},"source":["## 3.1. LeNet-V7\n","\n","In this part of the project we took our best LeNet class from the handcrafted model to try to hypertune the possible parameters in it. \n","\n","For this class, the search parameters defined were: \n","- number of filters in convolutional layers\n","- size of the kernerls in convolutional layers\n","- the type of the regularizer applied in the layers, in this point in specific we used the l2 regularization, but we control the learning rate applied to it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EW1YVFHUbTWQ"},"outputs":[],"source":["class LeNet_v7(tf.keras.Model):\n","    '''LeNet-V7 Enhanced Architecture + L2 Regularization (0.001) + Adam optimizer'''\n","    def __init__(self, seed=0,\n","                conv_1_filters = 6, \n","                conv_2_filters = 16,\n","                kernel = (5,5),\n","                regularizer = None\n","                 ):\n","        super().__init__()\n","        # Convolutional layers (with learnable parameters)\n","        self.resize = layers.Rescaling(1./255)\n","        self.conv_1 = layers.Conv2D(filters=conv_1_filters, kernel_size=kernel, \n","                                    kernel_initializer=initializers.GlorotNormal(seed=seed), padding=\"same\",\n","                                    kernel_regularizer=regularizer)\n","        self.conv_2 = layers.Conv2D(filters=conv_2_filters, kernel_size=kernel, \n","                                    kernel_initializer=initializers.GlorotNormal(seed=seed), padding=\"same\",\n","                                    kernel_regularizer=regularizer) \n","        self.dense_1 = layers.Dense(units=120, activation=\"relu\", kernel_regularizer=regularizer,\n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed))\n","        self.dense_2 = layers.Dense(units=84, activation=\"relu\", kernel_regularizer=regularizer,\n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed))\n","        self.output_layer = layers.Dense(units=1, activation=\"linear\", \n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed))\n","        # Non-learnable layers (define only once)\n","        self.relu = layers.Activation(\"relu\")\n","        self.maxpool2x2 = layers.MaxPooling2D(pool_size=2, strides=2)\n","        self.flatten = layers.Flatten()\n","        \n","    def call(self, inputs):\n","        # Orderly flows the inputs through the network's components\n","        x = self.resize(inputs)\n","        x = self.conv_1(x)\n","        x = self.relu(x)\n","        x = self.maxpool2x2(x)\n","        x = self.conv_2(x)\n","        x = self.relu(x)\n","        x = self.maxpool2x2(x)\n","        x = self.flatten(x)\n","        x = self.dense_1(x)\n","        x = self.dense_2(x)\n","        x = self.output_layer(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"DuF1nICNbTWR"},"source":["### 3.1.1. Setup tuner\n","\n","In this step, we will specify the hyperparameters to be tuned and define the search spaces, which determine the range of values that the hyperparameters can take during the tuning process.\n","\n","The parameters for LeNet best class: \n","\n","- number of filter applied in the first convolutional layer with values varying between 5 to 25 with step of 5.\n","- number of filter applied in the second convolutional layer with values varying between 20 to 40 with step of 5.\n","- sizes of the kernels varying in (3,3), (5,5) and (10,10).\n","- learning rates in 0.01 and 0.001."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtDaNVRCbTWR"},"outputs":[],"source":["def lenet_v7_builder(hp):\n","    conv_1_filters = hp.Int(\"conv_1_filters\", min_value=5, max_value=25, step=5)\n","    conv_2_filters = hp.Int(\"conv_2_filters\", min_value=20, max_value=40, step=5)\n","\n","    kernel_size = hp.Choice(\"kernel\", [3,5,10])\n","    kernel = (kernel_size, kernel_size)\n","\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n","\n","    model = LeNet_v7(\n","        seed=0,\n","        regularizer=regularizers.l2(hp_learning_rate),\n","        conv_1_filters=conv_1_filters,\n","        conv_2_filters=conv_2_filters,\n","        kernel=kernel\n","    )\n","    model.compile(\n","        optimizer=optimizers.Adam(learning_rate=hp_learning_rate),\n","        loss = losses.MeanSquaredError(),\n","        metrics=[metrics.MeanAbsolutePercentageError(name='MAPE')]\n","        )\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"jWMvLISGbTWR"},"source":["### 3.1.2. Build and start tuner \n","\n","In this step we'll set the folder to be saved all trials made in the hypertuning search. \n","\n","As we mentioned before we're using the `HyperBand class` from `keras_tuner`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQJnerKTbTWR"},"outputs":[],"source":["# Creates a new folder in hypertune, to save the AlexNet-V9 results and tuner.\n","project_name = \"lenet_v7_1\"\n","path_save_tuning_lenet = \"/content/drive/MyDrive/FacialAgeProject/models/hypertune/\" + project_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4ICLIfUbTWR"},"outputs":[],"source":["# Initialize the Hyperband tuner.\n","tuner_lenet = kt.Hyperband(lenet_v7_builder,\n","                            objective='val_loss',\n","                            max_epochs=10,\n","                            factor=3,\n","                            seed=seed,\n","                            directory=path_save_tuning_lenet,\n","                            project_name=project_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIoEafi5bTWR","outputId":"f4e4ae0d-6570-421e-e6ae-6c3d79d3d3fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 30 Complete [00h 02m 29s]\n","val_loss: 499.698486328125\n","\n","Best val_loss So Far: 103.35916137695312\n","Total elapsed time: 00h 27m 24s\n","INFO:tensorflow:Oracle triggered exit\n"]}],"source":["# Initialize early stopping on validation loss after 5 epochs.\n","stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Start the hyperparameter search.\n","history_tuner_lenet = tuner_lenet.search(\n","                                ds_train, \n","                                validation_data=ds_val,\n","                                epochs=epochs, \n","                                batch_size=batch_size,\n","                                callbacks=[stop_early]\n","                                )"]},{"cell_type":"markdown","metadata":{"id":"6NKSzQHaF8FU"},"source":["We reached with hypertuning the following parameters for LeNet:\n","\n","< MISSING THE PRINT OF THE BEST PARAMETERS FOR ALEXNET>\n","\n","With that we reached a validation loss of 103."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hPCe9RgF43b"},"outputs":[],"source":["tuner_lenet.get_best_hyperparameters(1)[0]"]},{"cell_type":"markdown","metadata":{"id":"EkrmjGY9bTWR"},"source":["## 3.2. AlexNet-V4\n","\n","In this part of the project we took our best AlexNet class from the handcrafted model to try to hypertune the possible parameters in it. \n","\n","For this class, the search parameters defined were: \n","- number of filters in convolutional layers\n","- size of the kernerls in convolutional layers\n","- the size of the strides\n","- the dropout rate\n","- quantity of units in dense layers\n","- the type of the regularizer applied in the layers, in this point in specific we used the l2 regularization, but we control the learning rate applied to it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyKmwzAmKWzA"},"outputs":[],"source":["class AlexNet_v4(tf.keras.Model):\n","    '''AlexNet-V4 Less Complex Architecture 1 + Dropout Rate (0.25) + L2 Regularization (0.001)'''\n","    def __init__(self, \n","                 regularizer=None, \n","                 seed=0,\n","                 conv1_filter=64,\n","                 conv2_filter=128,\n","                 conv3_filter=192,\n","                 conv4_filter=192,\n","                 conv5_filter=128,\n","                 kernel=(3,3),\n","                 strides=(2,2),\n","                 dense1_units=2048,\n","                 dense2_units=2048,\n","                 dropout_rate=0.25\n","                 ):\n","        super().__init__()\n","        \n","        # Rescaling.\n","        self.rescaling = layers.Rescaling(1./255)\n","\n","        # Convolutional layers (with learnable parameters).\n","        self.conv1 = layers.Conv2D(filters=conv1_filter, kernel_size=kernel, strides=strides, \n","                                    kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                    kernel_regularizer=regularizer)\n","        self.conv2 = layers.Conv2D(filters=conv2_filter, kernel_size=kernel, strides=strides, padding=\"same\",\n","                                 kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                 kernel_regularizer=regularizer)   \n","        self.conv3 = layers.Conv2D(filters=conv3_filter, kernel_size=kernel, strides=strides, padding=\"same\",\n","                                 kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                 kernel_regularizer=regularizer)\n","        self.conv4 = layers.Conv2D(filters=conv4_filter, kernel_size=kernel, strides=strides, padding=\"same\",\n","                                 kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                 kernel_regularizer=regularizer)\n","        self.conv5 = layers.Conv2D(filters=conv5_filter, kernel_size=kernel, strides=strides, padding=\"same\",\n","                                 kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                 kernel_regularizer=regularizer)\n","\n","        # Batch normalization layers (with learnable parameters, gamma and beta).\n","        self.bn0 = layers.BatchNormalization()\n","        self.bn1 = layers.BatchNormalization() \n","        self.bn2 = layers.BatchNormalization()\n","        self.bn3 = layers.BatchNormalization()\n","        self.bn4 = layers.BatchNormalization()\n","        self.bn5 = layers.BatchNormalization()\n","        \n","        # Classifier's head.\n","        self.dense1 = layers.Dense(units=dense1_units,\n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                   kernel_regularizer=regularizer)\n","        self.dense2 = layers.Dense(units=dense2_units,\n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                   kernel_regularizer=regularizer)\n","        self.output_layer = layers.Dense(units=1, activation=\"linear\", \n","                                   kernel_initializer=initializers.GlorotNormal(seed=seed),\n","                                   kernel_regularizer=regularizer)\n","        \n","        # Non-learnable layers (define only once)\n","        self.relu = layers.Activation(\"relu\")\n","        self.maxpool3x3 = layers.MaxPooling2D(pool_size=(3,3), strides=(2,2))\n","        self.dropout = layers.Dropout(dropout_rate)\n","        self.flatten = layers.Flatten()\n","\n","        \n","    def call(self, inputs):\n","        # Orderly flows the inputs through the network's components\n","        x = self.rescaling(inputs)\n","        x = self.bn0(x)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool3x3(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        x = self.maxpool3x3(x)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        x = self.relu(x)\n","        x = self.conv5(x)\n","        x = self.bn5(x)\n","        x = self.relu(x)\n","        x = self.flatten(x)\n","        x = self.dense1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.dense2(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.output_layer(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"YzBuzWHvbTWS"},"source":["### 3.2.1. Setup tuner\n","\n","In this step, we will specify the hyperparameters to be tuned and define the search spaces, which determine the range of values that the hyperparameters can take during the tuning process.\n","\n","As we saw in the behaviour of the handcrafted model, if we decrease the number of filters in the architecture of AlexNet we got a better result in terms of our metric, for this we decided to try to reduce more the filters in the convolutional layers, varying with a lowest number and increasing until the upper bound found in the hadcrafted model. We replicated this strategy for the quantity of units in the dense layers.\n","\n","We tried also to change the numbers of strides, but it was very costly, so we decided to maintain the standard value of 2.\n","\n","And for last, to control better the overfitting we choose to control the dropout rate putting the possible values as 0.05, 0.25 and 0.5.\n","\n","**The parameters for AlexNet best class:**\n","\n","- number of filter applied in the first convolutional layer with values varying between 32 to 128 with step of 8.\n","- number of filter applied in the second convolutional layer with values varying between 64 to 256 with step of 16.\n","- number of filter applied in the third convolutional layer with values varying between 96 to 384 with step of 32.\n","- number of filter applied in the fourth convolutional layer with values varying between 96 to 384 with step of 32.\n","- number of filter applied in the fifth convolutional layer with values varying between 64 to 256 with step of 16.\n","- sizes of the kernels varying in (3,3), (5,5) and (10,10).\n","- learning rates in 0.01 and 0.001.\n","- first dense units varying between 1024 to 2480 with steps of 256\n","- second dense units varying between 1024 to 2480 with steps of 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZapPVTJhxfAC"},"outputs":[],"source":["def alexnet_v4_builder(hp):\n","    conv1_filter = hp.Int('conv1_filter', min_value=32, max_value=128, step=8)\n","    conv2_filter = hp.Int('conv2_filter', min_value=64, max_value=256, step=16)\n","    conv3_filter = hp.Int('conv3_filter', min_value=192/2, max_value=192*2, step=32)\n","    conv4_filter = hp.Int('conv4_filter', min_value=192/2, max_value=192*2, step=32)\n","    conv5_filter = hp.Int('conv5_filter', min_value=64, max_value=256, step=16)\n","\n","    kernel_size = hp.Choice(\"kernel\", [3,4,5])\n","    kernel = (kernel_size, kernel_size)\n","\n","    stride_size = hp.Choice(\"strides\", [2])\n","    strides = (stride_size, stride_size)\n","\n","    dense1_units = hp.Int('dense1_units', min_value=1024, max_value=2480, step=256)\n","    dense2_units = hp.Int('dense2_units', min_value=1024, max_value=2480, step=256)\n","\n","    dropout_rate = hp.Choice('dropout_rate', values=[0.05, 0.25, 0.5])\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","    tf.keras.backend.clear_session()\n","\n","    model = AlexNet_v9(\n","        regularizer=regularizers.l2(hp_learning_rate),\n","        seed=0,\n","        conv1_filter=conv1_filter,\n","        conv2_filter=conv2_filter,\n","        conv3_filter=conv3_filter,\n","        conv4_filter=conv4_filter,\n","        conv5_filter=conv5_filter,\n","        kernel=kernel,\n","        strides=strides,\n","        dense1_units=dense1_units,\n","        dense2_units=dense2_units,\n","        dropout_rate=dropout_rate\n","    )\n","    model.compile(\n","        optimizer=optimizers.Adam(learning_rate=hp_learning_rate),\n","        loss = losses.MeanSquaredError(),\n","        metrics=[metrics.MeanAbsolutePercentageError(name='MAPE')]\n","        )\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"AyYXmZ7wbTWS"},"source":["### 3.2.2. Build and start tuner \n","\n","In this step we'll set the folder to be saved all trials made in the hypertuning search. \n","\n","As we mentioned before we're using the `HyperBand class` from `keras_tuner`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPStqxjUbTWS"},"outputs":[],"source":["# Creates a new folder in hypertune, to save the AlexNet-V9 results and tuner.\n","project_name = \"alexnet_v4\"\n","path_save_tuning_alexnet = \"/content/drive/MyDrive/FacialAgeProject/models/hypertune/\" + project_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmnfpMFe1znu"},"outputs":[],"source":["# Initialize the Hyperband tuner.\n","tuner_alexnet = kt.Hyperband(alexnet_v4_builder,\n","                            objective='val_loss',\n","                            max_epochs=10,\n","                            factor=3,\n","                            seed=seed,\n","                            directory=path_save_tuning_alexnet,\n","                            project_name=project_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzNOs0es5UwA","outputId":"a63f4cad-d761-4d3d-8bb8-54c5bfef18cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 30 Complete [00h 02m 15s]\n","val_loss: 84.8982162475586\n","\n","Best val_loss So Far: 82.37137603759766\n","Total elapsed time: 00h 27m 53s\n","INFO:tensorflow:Oracle triggered exit\n"]}],"source":["# Initialize early stopping on validation loss after 5 epochs.\n","stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Start the hyperparameter search.\n","history_tuner_alexnet = tuner_alexnet.search(\n","                        ds_train, \n","                        validation_data=ds_val,\n","                        epochs=epochs, \n","                        batch_size=batch_size,\n","                        callbacks=[stop_early]\n","                        )"]},{"cell_type":"markdown","metadata":{"id":"h0MLUvC0GQD0"},"source":["We reached with hypertuning the following parameters for AlexNet:\n","\n","< MISSING THE PRINT OF THE BEST PARAMETERS FOR ALEXNET>\n","\n","With that we reached a validation loss of 82 that was the best of our models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEt0VwlZGSw7"},"outputs":[],"source":["tuner_alexnet.get_best_hyperparameters(1)[0]"]},{"cell_type":"markdown","metadata":{"id":"YY05pmu-bTWS"},"source":["## 3.3. Train best models on full training dataset"]},{"cell_type":"markdown","metadata":{"id":"kwIn1ZXabTWT"},"source":["### 3.3.1. Create datasets\n","\n","We have to split again our full dataset in training and test dataset, without validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxvnhpZ8bTWT"},"outputs":[],"source":["# Data generators and parameters\n","train_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n","\n","test_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n","\n","generate_params = {\n","    'target_size' : (200,200),\n","    'color_mode' : 'rgb',\n","    'class_mode' : 'raw',\n","    'batch_size' : batch_size,\n","    'seed' : seed\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHLeX5tRbTWT","outputId":"97b9d8e2-2b2d-47fe-f145-94e75d565108"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7008 validated image filenames.\n","Found 1752 validated image filenames.\n"]}],"source":["train_images = train_generator.flow_from_dataframe(\n","    dataframe=train_images_df,\n","    x_col='file_name',\n","    y_col='age_label',\n","    shuffle=True,\n","    **generate_params\n",")\n","\n","test_images = test_generator.flow_from_dataframe(\n","    dataframe=test_images_df,\n","    x_col='file_name',\n","    y_col='age_label',\n","    shuffle=True,\n","    **generate_params\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjrPZkPKbTWT"},"outputs":[],"source":["# Define the save path of the models.\n","path_save_model =  \"/content/drive/MyDrive/FacialAgeProject/models/best_models\""]},{"cell_type":"markdown","metadata":{"id":"8a26jbhWbTWT"},"source":["### 3.3.2. Train LeNet-V7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OyTBYEKbTWT"},"outputs":[],"source":["# Get the best hyperparameters and build model.\n","best_hps_lenet = tuner_lenet.get_best_hyperparameters(1)[0]\n","best_lenet = tuner_lenet.hypermodel.build(best_hps_lenet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0c4JZjabTWT","outputId":"3aacc307-35c3-4f6e-ad3d-84ab1c0b5f07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training of LeNet-V7 Enhanced Architecture + L2 Regularization (0.001) + `Adam` optimizer \n","Epoch 1/10\n","110/110 [==============================] - 16s 145ms/step - loss: 372.7375 - MAPE: 287.1778 - val_loss: 257.7497 - val_MAPE: 221.4447\n","Epoch 2/10\n","110/110 [==============================] - 15s 136ms/step - loss: 199.5924 - MAPE: 134.3073 - val_loss: 181.1015 - val_MAPE: 103.4127\n","Epoch 3/10\n","110/110 [==============================] - 15s 132ms/step - loss: 149.9729 - MAPE: 101.8941 - val_loss: 151.6061 - val_MAPE: 102.3198\n","Epoch 4/10\n","110/110 [==============================] - 15s 136ms/step - loss: 119.2996 - MAPE: 83.0826 - val_loss: 130.0968 - val_MAPE: 71.6223\n","Epoch 5/10\n","110/110 [==============================] - 15s 137ms/step - loss: 105.0722 - MAPE: 76.4647 - val_loss: 118.7564 - val_MAPE: 76.1515\n","Epoch 6/10\n","110/110 [==============================] - 15s 133ms/step - loss: 85.9272 - MAPE: 67.0002 - val_loss: 113.6965 - val_MAPE: 72.1517\n","Epoch 7/10\n","110/110 [==============================] - 15s 134ms/step - loss: 74.5305 - MAPE: 58.3687 - val_loss: 106.2371 - val_MAPE: 64.0274\n","Epoch 8/10\n","110/110 [==============================] - 15s 135ms/step - loss: 67.3468 - MAPE: 56.8695 - val_loss: 118.8284 - val_MAPE: 69.6874\n","Epoch 9/10\n","110/110 [==============================] - 15s 136ms/step - loss: 52.8693 - MAPE: 49.3132 - val_loss: 121.7620 - val_MAPE: 81.4376\n","Epoch 10/10\n","110/110 [==============================] - 15s 135ms/step - loss: 45.7793 - MAPE: 44.3942 - val_loss: 105.6241 - val_MAPE: 60.8290\n","Training time: 151.0221s\n","\n"]}],"source":["history_lenet = train_best_model(model=best_lenet,\n","                                   training=train_images,\n","                                   test = test_images,\n","                                   epochs=epochs,\n","                                   batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bf4j1gkmbTWT","outputId":"e28bc50d-6410-44cf-ae0b-4a53732287ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models/LeNet-V7\\assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models/LeNet-V7\\assets\n"]},{"name":"stdout","output_type":"stream","text":["LeNet saved successfully into C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models\n"]}],"source":["# Saves the best model as .keras file.\n","history_lenet.model.save(filepath=path_save_model + \"/tuned_LeNet-V7\", \n","                    overwrite=True, \n","                    save_format=\"keras\")\n","\n","print(f\"LeNet saved successfully into {path_save_model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCnB61Z4bTWT","outputId":"d4f05cf3-db74-4aa2-cdeb-7c5b12630e58"},"outputs":[{"name":"stdout","output_type":"stream","text":["LeNet-V7-history.pkl saved successfully into C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models\n"]}],"source":["# Save the history dictionary of our best model.\n","name_file = 'LeNet-V7-history.pkl'\n","save_object = history_lenet.history\n","save_path = os.path.join(path_save_model, \"tuned_LeNet-V7\", name_file)\n","with open(save_path, 'wb') as fp:\n","    pickle.dump(save_object, fp)\n","    print(f'{name_file} saved successfully into {path_save_model}')"]},{"cell_type":"markdown","metadata":{"id":"Ewk8-mpHbTWU"},"source":["### 3.3.3. Train AlexNet-V4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jenI19UbTWU"},"outputs":[],"source":["# Get the best hyperparameters and build model.\n","best_hps_alexnet = tuner_alexnet.get_best_hyperparameters(1)[0]\n","best_alexnet = tuner_alexnet.hypermodel.build(best_hps_alexnet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvytmNGJbTWU","outputId":"446c1acb-7c2e-48bc-eeea-c20a1ad52aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start training of AlexNet-V9 Less Complex Architecture 1 + Dropout Rate (0.25) + L2 Regularization (0.001)\n","Epoch 1/10\n","110/110 [==============================] - 20s 178ms/step - loss: 221.1779 - MAPE: 127.3508 - val_loss: 397.5766 - val_MAPE: 90.0773\n","Epoch 2/10\n","110/110 [==============================] - 17s 152ms/step - loss: 134.0887 - MAPE: 75.5531 - val_loss: 153.3660 - val_MAPE: 111.8407\n","Epoch 3/10\n","110/110 [==============================] - 17s 155ms/step - loss: 101.9559 - MAPE: 62.0485 - val_loss: 220.9423 - val_MAPE: 47.5654\n","Epoch 4/10\n","110/110 [==============================] - 19s 170ms/step - loss: 85.5707 - MAPE: 55.1181 - val_loss: 148.7623 - val_MAPE: 49.1581\n","Epoch 5/10\n","110/110 [==============================] - 19s 174ms/step - loss: 73.3392 - MAPE: 47.9653 - val_loss: 103.4637 - val_MAPE: 57.7347\n","Epoch 6/10\n","110/110 [==============================] - 19s 168ms/step - loss: 60.2831 - MAPE: 42.7018 - val_loss: 95.9103 - val_MAPE: 74.7208\n","Epoch 7/10\n","110/110 [==============================] - 17s 150ms/step - loss: 55.8157 - MAPE: 40.0740 - val_loss: 78.2854 - val_MAPE: 47.7249\n","Epoch 8/10\n","110/110 [==============================] - 17s 150ms/step - loss: 43.5520 - MAPE: 35.0090 - val_loss: 85.5690 - val_MAPE: 47.3116\n","Epoch 9/10\n","110/110 [==============================] - 17s 150ms/step - loss: 40.8344 - MAPE: 31.9130 - val_loss: 87.7355 - val_MAPE: 41.3922\n","Epoch 10/10\n","110/110 [==============================] - 17s 150ms/step - loss: 37.1464 - MAPE: 32.6278 - val_loss: 95.6341 - val_MAPE: 41.1723\n","Training time: 177.9209s\n","\n"]}],"source":["history_alexnet = train_best_model(model=best_alexnet,\n","                                   training=train_images,\n","                                   test = test_images,\n","                                   epochs=epochs,\n","                                   batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvOOcv6dbTWU","outputId":"72d16601-ca7e-4f62-e5b0-7fe9a023c657"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models/AlexNet-V9\\assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models/AlexNet-V9\\assets\n"]},{"name":"stdout","output_type":"stream","text":["AlexNet saved successfully into C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models\n"]}],"source":["# Saves the best model as .keras file.\n","history_alexnet.model.save(filepath=path_save_model + \"/tuned_AlexNet-V4\", \n","                    overwrite=True, \n","                    save_format=\"keras\")\n","\n","print(f\"AlexNet saved successfully into {path_save_model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGugjcoZbTWU","outputId":"9e80eb10-bbec-472a-b1a2-16fec15b14f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["AlexNet-V9-history.pkl saved successfully into C:/Users/jkick/OneDrive - NOVAIMS/Sem. 2/Deep Learning/Project/FacialAgeProject/models/best_models\n"]}],"source":["# Save the history dictionary of our best model.\n","name_file = 'AlexNet-V4-history.pkl'\n","save_object = history_alexnet.history\n","save_path = os.path.join(path_save_model, \"tuned_AlexNet-V4\",  name_file)\n","with open(save_path, 'wb') as fp:\n","    pickle.dump(save_object, fp)\n","    print(f'{name_file} saved successfully into {path_save_model}')"]},{"cell_type":"markdown","metadata":{"id":"j2dC848sbTWU"},"source":["# 4. Conclusion\n","\n","Hyperparameter tuning is a crucial aspect of building high-performing machine learning models. As we observed during the hypertuning part, it can be time consuming and very costly to find the best solution.\n","\n","Another important consideration that we notice was to limit the search space to only the parameters that are most relevant to the problem at hand, with that we could reduce the search space and then run the hypertuning in time. \n","\n","We found that tuning the hyperparameters of our two best-performing models, LeNet and AlexNet, led to a slight improvement in their performance evidencing that it'll not be just the hypertuning phase that can solve the problems of a machine learning problem. It's necessary in this case find differents strategies to improve the models as well as different architectures, differents approaches as transfer learnings, etc."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
